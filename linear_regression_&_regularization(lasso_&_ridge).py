# -*- coding: utf-8 -*-
"""Linear Regression & Regularization(Lasso & Ridge).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vM-comBLT09LHwp_O11SGE_ON1L5YcBx

##Necessary imports
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings('ignore')

# %matplotlib inline
plt.style.use('fivethirtyeight')

"""##Load the data"""

df = pd.read_csv("/content/HousingData.csv")
df

df.describe()

df.info()

# looking at null values
df.isna().sum()

df['CRIM'].fillna(df['CRIM'].median(), inplace=True)
df['ZN'].fillna(df['ZN'].median(), inplace=True)
df['INDUS'].fillna(df['INDUS'].median(), inplace=True)
df['CHAS'].fillna(df['CHAS'].median(), inplace=True)
df['AGE'].fillna(df['AGE'].median(), inplace=True)
df['LSTAT'].fillna(df['LSTAT'].median(), inplace=True)

df.isna().sum()

"""##EDA"""

# Let's see how data is distributed for every column
plt.figure(figsize = (20, 15))
plotnumber = 1

for column in df:
    if plotnumber <= 14:
        ax = plt.subplot(3, 5, plotnumber)
        sns.distplot(df[column])
        plt.xlabel(column, fontsize = 15)

    plotnumber += 1

plt.tight_layout()
plt.show()

# Plotting `Price` with remaining columns
plt.figure(figsize = (20, 15))
plotnumber = 1

for column in df:
    if plotnumber <= 14:
        ax = plt.subplot(3, 5, plotnumber)
        sns.scatterplot(x = df['MEDV'], y = df[column])

    plotnumber += 1

plt.tight_layout()
plt.show()

# looking for outliers using box plot
plt.figure(figsize = (20, 8))
sns.boxplot(data = df, width = 0.8)
plt.show()

# creating features and label variable

X = df.drop(columns = 'MEDV', axis = 1)
y = df['MEDV']

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_scaled

# checking for multicollinearity using `VIF` and `correlation matrix`

from statsmodels.stats.outliers_influence import variance_inflation_factor

vif = pd.DataFrame()

vif['VIF'] = [variance_inflation_factor(X_scaled, i) for i in range(X_scaled.shape[1])]
vif['Features'] = X.columns

vif

# dropping 'TAX' column from data

X.drop(columns = ['TAX'], axis = 1)

# Heatmap

fig, ax = plt.subplots(figsize = (16, 8))
sns.heatmap(df.corr(), annot = True, fmt = '1.2f', annot_kws = {'size' : 10}, linewidth = 1)
plt.show()

import statsmodels.formula.api as smf

lm = smf.ols(formula = 'MEDV ~ RAD', data = df).fit()
lm = smf.ols(formula = 'MEDV ~ TAX', data = df).fit()
lm.summary()

# removing "RAD" column
df.drop(columns = 'RAD', axis = 1, inplace = True)

df.head()

"""##Traing and Testing"""

# splitting data into training asnd test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.30, random_state = 0)

"""##Model fitting"""

# fitting training data to model

from sklearn.linear_model import LinearRegression

lr = LinearRegression()
lr.fit(X_train, y_train)

# prediction of model
y_pred = lr.predict(X_test)

# training accuracy of model
lr.score(X_train, y_train)

# test accuracy of model
lr.score(X_test, y_test)

# creating a function to create adhusted R-Squared

def adj_r2(X, y, model):
    r2 = model.score(X, y)
    n = X.shape[0]
    p = X.shape[1]
    adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)
    return adjusted_r2

print(adj_r2(X_train, y_train, lr))

print(adj_r2(X_test, y_test, lr))

"""Model r2 score is less on the test data so there is chance of overfitting, Check this using regularization.

##Lasso Regression
"""

from sklearn.linear_model import Lasso, LassoCV
lasso_cv = LassoCV(alphas = None, cv = 10, max_iter = 100000)
lasso_cv.fit(X_train, y_train)

# best alpha parameter
alpha = lasso_cv.alpha_
alpha

lasso = Lasso(alpha = lasso_cv.alpha_)
lasso.fit(X_train, y_train)

lasso.score(X_train, y_train)

lasso.score(X_train, y_train)

print(adj_r2(X_train, y_train, lasso))

print(adj_r2(X_test, y_test, lasso))

"""##Ridge Regression"""

from sklearn.linear_model import Ridge, RidgeCV

alphas = np.random.uniform(0, 10, 50)
ridge_cv = RidgeCV(alphas = alphas, cv = 10)
ridge_cv.fit(X_train, y_train)

# best alpha parameter
alpha = ridge_cv.alpha_
alpha

ridge = Ridge(alpha = ridge_cv.alpha_)
ridge.fit(X_train, y_train)

ridge.score(X_train, y_train)

ridge.score(X_test, y_test)

print(adj_r2(X_train, y_train, ridge))

print(adj_r2(X_test, y_test, ridge))

"""##Elastic Net


"""

from sklearn.linear_model import ElasticNet, ElasticNetCV

elastic_net_cv = ElasticNetCV(alphas = None, cv = 10, max_iter = 100000)
elastic_net_cv.fit(X_train, y_train)

# best alpha parameter
alpha = elastic_net_cv.alpha_
alpha

# l1 ratio
elastic_net_cv.l1_ratio

elastic_net = ElasticNet(alpha = elastic_net_cv.alpha_, l1_ratio = elastic_net_cv.l1_ratio)
elastic_net.fit(X_train, y_train)

elastic_net.score(X_train, y_train)

elastic_net.score(X_test, y_test)

print(adj_r2(X_train, y_train, elastic_net))

"""We still are getting the same r2 score. That means our Regression model has been well trained over the training data and there is no overfitting."""